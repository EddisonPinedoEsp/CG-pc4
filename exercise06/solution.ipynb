{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "from ultralytics import YOLO\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def highlight_people_cars_and_bikes(full_path_input_image, \n",
    "                                  color_scale_image, \n",
    "                                  color_scale_people, \n",
    "                                  color_scale_cars, \n",
    "                                  color_scale_bikes, \n",
    "                                  full_path_output_image):\n",
    "    # Cargar modelos\n",
    "    yolo_model = YOLO('yolov8x.pt')\n",
    "    sam = sam_model_registry[\"default\"](\"sam_vit_h_4b8939.pth\")\n",
    "    sam_predictor = SamPredictor(sam)\n",
    "    \n",
    "    # Cargar y convertir imagen a escala de grises\n",
    "    image = cv2.imread(full_path_input_image)\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    colored_image = cv2.cvtColor(gray_image, cv2.COLOR_GRAY2BGR)\n",
    "    \n",
    "    # Convertir imagen base al color_scale_image\n",
    "    colored_image = cv2.multiply(colored_image, \n",
    "                               np.array(color_scale_image)/255.0)\n",
    "    \n",
    "    # Detectar objetos con YOLO\n",
    "    results = yolo_model(image)\n",
    "    \n",
    "    # Preparar imagen para SAM\n",
    "    sam_predictor.set_image(image)\n",
    "    \n",
    "    for result in results[0]:\n",
    "        bbox = result.boxes.xyxy[0].cpu().numpy()\n",
    "        class_id = int(result.boxes.cls[0].item())\n",
    "        conf = result.boxes.conf[0].item()\n",
    "        \n",
    "        if conf < 0.5:  # Umbral de confianza\n",
    "            continue\n",
    "            \n",
    "        # Convertir bbox a formato input_box para SAM\n",
    "        input_box = np.array([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
    "        \n",
    "        # Obtener máscara de SAM\n",
    "        masks, _, _ = sam_predictor.predict(\n",
    "            box=input_box,\n",
    "            multimask_output=False\n",
    "        )\n",
    "        mask = masks[0]\n",
    "        \n",
    "        # Aplicar color según la clase\n",
    "        if class_id == 0:  # persona\n",
    "            color = np.array(color_scale_people)/255.0\n",
    "        elif class_id == 2:  # coche\n",
    "            color = np.array(color_scale_cars)/255.0\n",
    "        elif class_id == 1:  # bicicleta/moto\n",
    "            color = np.array(color_scale_bikes)/255.0\n",
    "        else:\n",
    "            continue\n",
    "            \n",
    "        # Aplicar color solo en los píxeles de la máscara\n",
    "        colored_image[mask] = image[mask] * color\n",
    "    \n",
    "    # Guardar imagen resultante\n",
    "    cv2.imwrite(full_path_output_image, colored_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mEl kernel se bloqueó al ejecutar código en la celda actual o en una celda anterior. \n",
      "\u001b[1;31mRevise el código de las celdas para identificar una posible causa del error. \n",
      "\u001b[1;31mHaga clic <a href='https://aka.ms/vscodeJupyterKernelCrash'>aquí</a> para obtener más información. \n",
      "\u001b[1;31mVea Jupyter <a href='command:jupyter.viewOutput'>log</a> para obtener más detalles."
     ]
    }
   ],
   "source": [
    "# Ejemplo de colores en formato BGR\n",
    "highlight_people_cars_and_bikes(\n",
    "    'p1.jpg',\n",
    "    color_scale_image=[0.5, 0.5, 0.5],  # Gris\n",
    "    color_scale_people=[0, 0, 1],        # Rojo\n",
    "    color_scale_cars=[0, 1, 0],          # Verde\n",
    "    color_scale_bikes=[1, 0, 0],         # Azul\n",
    "    full_path_output_image = 'output.jpg'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
